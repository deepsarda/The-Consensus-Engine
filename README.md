Multi-Modal Misinformation Detection with Explanation Generation
Problem Statement Description
Misinformation increasingly spreads through multi-modal content—a mix of text, images, and videos—making detection significantly harder. A single modality may appear legitimate, while cross-modal inconsistencies reveal manipulation.
The task is to build a multi-modal AI system that detects misinformation and generates human-understandable explanations, while remaining robust against adversarial attempts and unseen manipulation techniques.
Mandatory Deliverables
1.	Multi-modal input handling (text + image and/or video)
2.	Detection of cross-modal inconsistencies (caption vs media mismatch)
3.	Identification of out-of-context media reuse
4.	Detection of AI-generated synthetic media (text, images, deepfakes)
5.	Natural language explanation generation citing concrete evidence
6.	Robustness against adversarial perturbations
7.	Quantitative evaluation (accuracy, robustness, explanation quality)

